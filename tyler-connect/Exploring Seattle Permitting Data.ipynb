{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Seattle Building Permit Data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started with any data, we need to make sure our environment has all the Python dependencies we need. In the cell below, we'll use the `pip install` command to install those dependencies. Typically, you might run this command directly from a command-line in a terminal application. You can get the same effect from within a Jupyter notebook by prepending an exaclamation point `!` to that command, which indicates to the interpreter that the command should be passed through to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we attempted to install these dependencies on all lab machines;\n",
    "# you should only need to execute this cell if you're running the notebook in Google Colab\n",
    "!wget http://download.osgeo.org/libspatialindex/spatialindex-src-1.8.5.tar.gz\n",
    "!tar -zxf spatialindex-src-1.8.5.tar.gz\n",
    "!cd spatialindex-src-1.8.5\n",
    "!cd spatialindex-src-1.8.5 && ./configure && make && make install\n",
    "!ldconfig\n",
    "!pip install pandas==0.24.2 statsmodels==0.9.0 rtree geopandas shapely sodapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets into our notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with two datasets as the basis of our analysis. We'll load a city of Seattle permits dataset as well as a shapefile with polygons defined for each of the neighborhoods in the state of Washington. We download these datasets into pandas DataFrames using the open source sodapy Python Socrata module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "\n",
    "def download_dataset(domain, dataset_id):\n",
    "    # for this exercise, we're not using an app token,\n",
    "    # but you *should* sign-up and register for an app_token if you want to use the Socrata API\n",
    "    client = Socrata(domain, app_token=None)\n",
    "    offset = None\n",
    "    data = []\n",
    "    batch_size = 1000\n",
    "\n",
    "    while True:\n",
    "        records = client.get(dataset_id, offset=offset, limit=batch_size)\n",
    "        data.extend(records)\n",
    "        if len(records) < batch_size:\n",
    "            break\n",
    "        offset = offset + batch_size if (offset) else batch_size\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def download_permits_dataset():\n",
    "    return seattle_permits_df if \"seattle_permits_df\" in globals() else download_dataset(\"data.seattle.gov\", \"k44w-2dcq\")\n",
    "\n",
    "def download_neighborhoods_dataset():\n",
    "    return wa_neighborhoods_df if \"wa_neighborhoods_df\" in globals() else download_dataset(\"robo.demo.socrata.com\", \"smef-bsgr\")\n",
    "\n",
    "# load Seattle permits data\n",
    "seattle_permits_df = download_permits_dataset()\n",
    "\n",
    "# load shapefile data\n",
    "wa_neighborhoods_df = download_neighborhoods_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells we'll do some exploration of our datasets using the `len`, `head`, and `value_counts` functions. We'll start by getting a sense of how many rows are in each of our datasets with the `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seattle_permits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wa_neighborhoods_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see have a peek at the first 5 rows in each of those dataset using the `head` method. You can optionally pass a parameter for the number of rows you want to print if 5 isn't enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_permits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_neighborhoods_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing our dataframes like this gives us a sense of what columns exist, as well as the frequently occurring values in the dataset. But there's an even better way to detrmine the top values for a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_permits_df[\"applieddate\"].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value counts make it clear that a lot of the value in the \"applieddate\" column are missing or null. Since the crux of our analysis requires time and location data, we need to handle those missing values. There a variety of ways you can handle missing data, but removing incomplete rows is the simplest, so it's what we'll do here today. In the next cell, we'll remove null dates and null latitude and longitude columns. There are also a lot of columns in the permits dataset that we won't use in this analysis. So we'll also filter down our dataset to just the columns we're interested in to reduce the amount of extraneous information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_permits_df = seattle_permits_df[seattle_permits_df[\"applieddate\"].notnull()]\n",
    "seattle_permits_df = seattle_permits_df[seattle_permits_df[\"latitude\"].notnull()]\n",
    "seattle_permits_df = seattle_permits_df[seattle_permits_df[\"longitude\"].notnull()]\n",
    "seattle_permits_df = seattle_permits_df[[\"applieddate\", \"latitude\", \"longitude\"]].reset_index(drop=True)\n",
    "seattle_permits_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing place-based data with Socrata Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and visualizing geographical data is relatively straight-forward in the Socrata UI. Before doing any additional work in Python, let's embed a map created in Socrata using its embed link. You'll notice that since the map is embedded within an IFrame (using iPython's IFrame class), that it's dynamic and we have preserved all the functionality from the map experience in the platform like changing the zoom, and searching for data within the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://robo.demo.socrata.com/dataset/Seattle-Building-Permits-Point-Map/7unc-ff4h/embed?width=800&height=600', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map above is a point map. Each of the rows from our permit dataset is rendered as a point on the map. Any observations about this visualization?\n",
    "\n",
    "I think we can do better with a multi-layer choropleth map -- also created within the Socrata platform -- where we aggregate our points based on the neighborhoods that they fall within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://robo.demo.socrata.com/dataset/Seattle-Building-Permits-Choropleth/i6rm-ys8r/embed?width=800&height=600\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any observations about this choropleth map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping our geographic columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in our choropleth map, we'll try to group our data by neighborhood, and we'll try to improve on our map by looking at how our permit applications change over time.\n",
    "\n",
    "Notice that the neighborhood shapefile we imported from Socrata contained boundaries for all neighborhoods in the state of Washington. (Can you use the the `value_counts` method to determine the top cities in the `wa_neighborhoods_df` dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you use the the value_counts method to determine the top cities in the wa_neighborhoods_df dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter that down to the city of Seattle, since that's the focus of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_neighborhoods_df = wa_neighborhoods_df[wa_neighborhoods_df[\"city\"] == \"Seattle\"].reset_index(drop=True)\n",
    "seattle_neighborhoods_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you use the `len` method to determine the number of rows in our new seattle_neighborhoods_df dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the really powerful features of notebooks is that you can execute arbitrary code to transform your data. Since data wrangling is one of the fundamental tasks in any data analysis project, it's essential that we have tools to reshape and slice our data. This is one of the strengths of the pandas in Python.\n",
    "\n",
    "We have already loaded the underlying neighborhood shapes, but let's make our coordinate and shape columns a little more useful by changing them to types that are a little easier to work with in Python. We'll make use of the geopandas and shapely Python modules. Why are we doing this? We want a single geographic column in each of our datasets, and in order to perform a join on those geographic columns, we need them to be respresented as types that our join function understands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we'll add a new Polygon column to our shapefile dataset. We'll write a function that takes a shape that's encoded in the geojson format, and transform it into a Polygon column. We'll use the `apply` method to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def polygon_from_geojson(geojson):\n",
    "    return Polygon(geojson[\"coordinates\"][0][0])\n",
    "\n",
    "seattle_neighborhoods_df[\"polygon\"] = seattle_neighborhoods_df[\"the_geom\"].apply(polygon_from_geojson)\n",
    "seattle_neighborhoods_df[\"polygon\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add a new Point column to our permits dataset. We write a function that takes an entire row from our dataset as an input, pulls out our latitude and longitude from that row, and returns a Point corresponding to those coordinates. Once again, we'll use the `apply` method to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "# create a new point column from latitude and longitude columns in permits dataset\n",
    "def point_from_coordinates(row):\n",
    "    coordinates = (row[\"longitude\"], row[\"latitude\"])\n",
    "    return Point(coordinates)\n",
    "\n",
    "seattle_permits_df[\"latitude\"] = pd.to_numeric(seattle_permits_df[\"latitude\"])\n",
    "seattle_permits_df[\"longitude\"] = pd.to_numeric(seattle_permits_df[\"longitude\"])\n",
    "seattle_permits_df[\"point\"] = seattle_permits_df.apply(point_from_coordinates, axis=1)\n",
    "seattle_permits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a spatial join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we our landuse dataframe and our neighborhoods dataframe, each with geometry columns, that next thing we want to do is perform a spatial join so we can have a single dataframe tying permits to neighborhoods. This will allow us to compare across neighborhoods, just as we did with the heat map earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "permits_geo_df = gpd.GeoDataFrame(seattle_permits_df, geometry='point')\n",
    "neighborhoods_geo_df = gpd.GeoDataFrame(seattle_neighborhoods_df, geometry='polygon')\n",
    "\n",
    "joined_df = pd.DataFrame(gpd.sjoin(permits_geo_df, neighborhoods_geo_df, how=\"inner\", op=\"within\", rsuffix=\"neighborhood_\"))\n",
    "joined_df = joined_df[[\"applieddate\", \"name\"]]#.sort_values(\"applieddate\")  # ROBERT TODO: can I remove the sort?\n",
    "\n",
    "## TODO: print the first rows of the joined data frame `joined_df`\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating based on neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! Now that we have a single dataset with each row corresponding to a permit and its corresponding neighborhood, we can group our dataset by neighborhood, which is the first step to exploring the number of permits by neighborhood. To make our neighborhood-based analysis a little more digestible, we'll restrict it to a subset of Seattle neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_of_interest = set([\"Ballard\", \"Capitol Hill\", \"Magnolia\", \"South Lake Union\", \"North Beacon Hill\", \"Pioneer Square\"])\n",
    "\n",
    "def is_neighborhood_of_interest(neighborhood):\n",
    "    return neighborhood in neighborhoods_of_interest\n",
    "\n",
    "subset_df = joined_df[joined_df[\"name\"].apply(is_neighborhood_of_interest)].reset_index(drop=True)\n",
    "subset_df.groupby(\"name\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating based on neighborhood and date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having grouped our data based on neighborhood, the next thing we want to do is look at one aspect of the data that was missing from the choropleth map -- time. Just as we did with the `Point` and `Polygon` columns previously, we need to convert the `applieddate` column type in the permits dataframe to a `datetime` in order to benefit from some time-based functionality in Python. We'll also filter out dates before 2005 and during 2019 so the years are comparable. We also make the new `datetime` column the index column for our dataframe, which gives us some added benefits such as automatic sorting based on date and good default behavior when plotting our time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def is_between_2005_and_2018(date):\n",
    "    return date > datetime.datetime(2005, 1, 1) and date < datetime.datetime(2019, 1, 1)\n",
    "\n",
    "fixed_dates_df = subset_df.copy()\n",
    "fixed_dates_df[\"applieddate\"] = subset_df[\"applieddate\"].apply(pd.to_datetime)\n",
    "fixed_dates_df = fixed_dates_df[fixed_dates_df[\"applieddate\"].apply(is_between_2005_and_2018)]\n",
    "fixed_dates_df = fixed_dates_df.set_index(fixed_dates_df[\"applieddate\"]).drop([\"applieddate\"], axis=1)\n",
    "grouped = fixed_dates_df.groupby(\"name\").resample(\"M\").count()\n",
    "grouped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with our data grouped by neighborhood and by month, let's visualize it! The code below is somewhat complex, so we won't go into it in detail. But the gist is this: we iterate over our groups (neighborhoods) and create a time series plot for each, labeling each plot based on its name, and coloring our line purple just because who doesn't like purple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "ncols=2\n",
    "nrows=3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15))\n",
    "\n",
    "for (name, neighborhood_df), axis in zip(grouped.groupby(level=0), axes.flatten()):\n",
    "    axis.xaxis.label.set_visible(False)\n",
    "    axis.set_ylim(0, 75)\n",
    "    axis.set_xlim(left=datetime.date(2000, 1, 1), right=datetime.date(2018, 12, 31))\n",
    "    axis.yaxis.set_major_formatter(FormatStrFormatter('%g'))\n",
    "    neighborhood_df.xs(name).plot(title=name, style=\"\", ax=axis, legend=False, color=\"purple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your observations based on these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding trend and seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select one of these neighboorhoods to drill in on. I'm partial to Ballard.\n",
    "\n",
    "The statsmodels package in Python gives us a really useful set of tools for analyzing time series data. In particular, we can use the `seasonal_decompose` method to breakdown a time series into consituent series corresponding to the longterm trend in the data, the seasonal aspect of the data, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "name, neighborhood_df = list(grouped.groupby(level=0))[0] # Ballard\n",
    "result = seasonal_decompose(neighborhood_df.xs(name))\n",
    "fig = result.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
